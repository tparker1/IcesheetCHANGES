{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Missing Files\n",
    "1. Parse the NASA site to get a list of available files from the satellite (eg. AquaMODIS, AquaTERRA SeaWiFS) \n",
    "2. Store the list of file names as a .txt\n",
    "3. Get list of availible files that are not in the filepath\n",
    "4. Download files that aren't on disk \n",
    "<br>\n",
    "\n",
    "[File Search: Using the API](https://oceandata.sci.gsfc.nasa.gov/api/file_search_help) \n",
    "\n",
    "\n",
    "[OceanData API file search GUI](https://oceandata.sci.gsfc.nasa.gov/api/file_search/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os       # Miscellaneous operating system interfaces\n",
    "import glob     # Unix style pathname pattern expansion\n",
    "import requests # HTTP library for Python\n",
    "import getpass  # Portable password input"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set search filters and file options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~~~~~~~~~~~~~~~\n",
    "## FILE OPTIONS\n",
    "# ~~~~~~~~~~~~~~~\n",
    "\n",
    "# SET path to a folder to store the data \n",
    "filepath = \"/Volumes/Seagate/SeaWiFs/chla/daily/\"\n",
    "\n",
    "# File name/path for availible file names output (temp file)\n",
    "file = filepath + \"file_list.txt\"\n",
    "\n",
    "\n",
    "# ~~~~~~~~~~~~~~~\n",
    "## SEARCH OPTIONS\n",
    "# ~~~~~~~~~~~~~~~\n",
    "# Date Range \n",
    "# YYYY-MM-DD\n",
    "\n",
    "# SET start date\n",
    "#start_date = \"2002-07-04\" # first modis dat\n",
    "#start_date = \"1997-09-04\"  # first SeaWiFS date\n",
    "start_date = \"2010-10-01\"\n",
    "\n",
    "# SET end date \n",
    "#end_date = \"2023-01-31\"   # last modis date\n",
    "#end_date = \"2010-12-11\"  # last SeaWiFS date\n",
    "end_date = \"2023-01-01\"\n",
    "\n",
    "# Resolution\n",
    "# 4km or 9km\n",
    "resolution = \"9km\"\n",
    "\n",
    "# SET Period (use empty string for all periods)\n",
    "# Daily = DAY\n",
    "# Monthly = MO\n",
    "#period = \"DAY\"\n",
    "period = \"DAY\"\n",
    "\n",
    "\n",
    "# ~~~~~~~~~~~~~~~\n",
    "# INSTRUMENT OPTS\n",
    "# ~~~~~~~~~~~~~~~\n",
    "\n",
    "# Dictionary of instruments and their sensor/dt ids (incomplete dictionary)\n",
    "instruments = {\n",
    "    \"aqua\": \"sensor_id=7&dtid=1043\",\n",
    "    \"terra\": \"sensor_id=8&dtid=1083\",\n",
    "    \"seaWiFS\": \"sensor_id=6&dtid=1123\"\n",
    "}\n",
    "\n",
    "# SET instrument\n",
    "inst_key = \"seaWiFS\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validity check\n",
    "# Check if intruments dictionary contains inst_name as a key\n",
    "if inst_key in instruments:\n",
    "    # Get the sensor/dt id from the dictionary based on inst_name\n",
    "    instrument = instruments[inst_key]\n",
    "else:\n",
    "    print(\"Invalid instrument name: \" + inst_key + \" not found in instruments dictionary.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provide app key or login information for Earth Data access. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How would you like to access the data?\n",
      "1. App Key\n",
      "2. Login\n",
      "An appkey can be obtained from: https://oceandata.sci.gsfc.nasa.gov/appkey/\n",
      "Please enter appkey now.\n"
     ]
    }
   ],
   "source": [
    "### Provide app key for Earth Data access.\n",
    "def get_appkey():\n",
    "    # An appkey can be obtained from:\n",
    "    # https://oceandata.sci.gsfc.nasa.gov/appkey/\n",
    "    print(\n",
    "        'An appkey can be obtained from: https://oceandata.sci.gsfc.nasa.gov/appkey/\\nPlease enter appkey now.'\n",
    "    )\n",
    "    appKey = getpass.getpass('Enter Appkey: ')\n",
    "    return appKey\n",
    "\n",
    "\n",
    "# getpass attempts to hide login information from the terminal\n",
    "def get_login():\n",
    "    print('Please enter your EarthData login information.')\n",
    "    #print('Enter Username: ')\n",
    "    user = input(\"Enter Username:\")\n",
    "    #print('Enter Password: ')\n",
    "    password = getpass.getpass(\"Enter Password: \")\n",
    "    return user, password\n",
    "\n",
    "\n",
    "# ~~~~~~~~~~~~~~~\n",
    "# Get user choice\n",
    "# ~~~~~~~~~~~~~~~\n",
    "\n",
    "# Get user choice for appkey or login\n",
    "choice = \"0\"\n",
    "while choice != \"1\" and choice != \"2\":\n",
    "    # Ask user for appkey or login\n",
    "    print(\"How would you like to access the data?\")\n",
    "    print(\"1. App Key\")\n",
    "    print(\"2. Login\")\n",
    "    choice = input(\"Enter 1 or 2: \")\n",
    "\n",
    "# Get appkey/login\n",
    "if choice == \"1\":\n",
    "    # Provide app key for Earth Data access.\n",
    "    appKey = get_appkey()\n",
    "elif choice == \"2\":\n",
    "    # Get username and password for Earth Data access.\n",
    "    username, password = get_login()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipe availible files from search to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  4177  100  4031  100   146   1759     63  0:00:02  0:00:02 --:--:--  1824\n"
     ]
    }
   ],
   "source": [
    "# If the filepath does not exist, create it\n",
    "if not os.path.exists(filepath):\n",
    "    os.makedirs(filepath)\n",
    "\n",
    "# Form commands using given search filters\n",
    "if period != \"\":\n",
    "    period = \"&period=\" + period\n",
    "#wget = \"wget -q --post-data=\\\"results_as_file=1&\" + instrument + \"&sdate=\" + start_date + \" 00:00:00&edate=\" + end_date + \" 23:59:59&subType=1&prod_id=chlor_a&resolution_id=\" + resolution + period + \"\\\" -O - https://oceandata.sci.gsfc.nasa.gov/api/file_search\"\n",
    "curl = \"curl -d \\\"results_as_file=1&\" + instrument + \"&sdate=\" + start_date + \" 00:00:00&edate=\" + end_date + \" 23:59:59&subType=1&prod_id=chlor_a&resolution_id=\" + resolution + period + \"\\\" https://oceandata.sci.gsfc.nasa.gov/api/file_search > \" + file \n",
    "\n",
    "# Execute command to retrieve list of files and store in file\n",
    "os.system(curl)\n",
    "\n",
    "# Check the file for \"ERROR\" or \"No Results Found\" and print an error message if found\n",
    "with open(file) as f:\n",
    "    for line in f:\n",
    "        if \"ERROR\" in line:\n",
    "            print(\"Error encountered while searching for files.\\nPlease verify search parameters and try again.\")\n",
    "        if \"No Results Found\" in line:\n",
    "            print(\"No results found.\\nPlease verify search parameters and try again.\\nExiting program.\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get list of availible files that are not already in the specified file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SEASTAR_SEAWIFS_GAC.20101001.L3m.DAY.CHL.chlor_a.9km.nc\\n', 'SEASTAR_SEAWIFS_GAC.20101002.L3m.DAY.CHL.chlor_a.9km.nc\\n', 'SEASTAR_SEAWIFS_GAC.20101003.L3m.DAY.CHL.chlor_a.9km.nc\\n', 'SEASTAR_SEAWIFS_GAC.20101004.L3m.DAY.CHL.chlor_a.9km.nc\\n', 'SEASTAR_SEAWIFS_GAC.20101005.L3m.DAY.CHL.chlor_a.9km.nc\\n', 'SEASTAR_SEAWIFS_GAC.20101006.L3m.DAY.CHL.chlor_a.9km.nc\\n', 'SEASTAR_SEAWIFS_GAC.20101007.L3m.DAY.CHL.chlor_a.9km.nc\\n', 'SEASTAR_SEAWIFS_GAC.20101008.L3m.DAY.CHL.chlor_a.9km.nc\\n', 'SEASTAR_SEAWIFS_GAC.20101009.L3m.DAY.CHL.chlor_a.9km.nc\\n', 'SEASTAR_SEAWIFS_GAC.20101010.L3m.DAY.CHL.chlor_a.9km.nc\\n', 'SEASTAR_SEAWIFS_GAC.20101011.L3m.DAY.CHL.chlor_a.9km.nc\\n', 'SEASTAR_SEAWIFS_GAC.20101012.L3m.DAY.CHL.chlor_a.9km.nc\\n', 'SEASTAR_SEAWIFS_GAC.20101013.L3m.DAY.CHL.chlor_a.9km.nc\\n', 'SEASTAR_SEAWIFS_GAC.20101014.L3m.DAY.CHL.chlor_a.9km.nc\\n', 'SEASTAR_SEAWIFS_GAC.20101015.L3m.DAY.CHL.chlor_a.9km.nc\\n', 'SEASTAR_SEAWIFS_GAC.20101016.L3m.DAY.CHL.chlor_a.9km.nc\\n', 'SEASTAR_SEAWIFS_GAC.20101017.L3m.DAY.CHL.chlor_a.9km.nc\\n', 'SEASTAR_SEAWIFS_GAC.20101018.L3m.DAY.CHL.chlor_a.9km.nc\\n', 'SEASTAR_SEAWIFS_GAC.20101019.L3m.DAY.CHL.chlor_a.9km.nc\\n', 'SEASTAR_SEAWIFS_GAC.20101020.L3m.DAY.CHL.chlor_a.9km.nc\\n', 'SEASTAR_SEAWIFS_GAC.20101021.L3m.DAY.CHL.chlor_a.9km.nc\\n', 'SEASTAR_SEAWIFS_GAC.20101022.L3m.DAY.CHL.chlor_a.9km.nc\\n', 'SEASTAR_SEAWIFS_GAC.20101023.L3m.DAY.CHL.chlor_a.9km.nc\\n', 'SEASTAR_SEAWIFS_GAC.20101024.L3m.DAY.CHL.chlor_a.9km.nc\\n', 'SEASTAR_SEAWIFS_GAC.20101025.L3m.DAY.CHL.chlor_a.9km.nc\\n', 'SEASTAR_SEAWIFS_GAC.20101026.L3m.DAY.CHL.chlor_a.9km.nc\\n', 'SEASTAR_SEAWIFS_GAC.20101027.L3m.DAY.CHL.chlor_a.9km.nc\\n', 'SEASTAR_SEAWIFS_GAC.20101028.L3m.DAY.CHL.chlor_a.9km.nc\\n', 'SEASTAR_SEAWIFS_GAC.20101029.L3m.DAY.CHL.chlor_a.9km.nc\\n', 'SEASTAR_SEAWIFS_GAC.20101030.L3m.DAY.CHL.chlor_a.9km.nc\\n', 'SEASTAR_SEAWIFS_GAC.20101031.L3m.DAY.CHL.chlor_a.9km.nc\\n', 'SEASTAR_SEAWIFS_GAC.20101101.L3m.DAY.CHL.chlor_a.9km.nc\\n', 'SEASTAR_SEAWIFS_GAC.20101102.L3m.DAY.CHL.chlor_a.9km.nc\\n', 'SEASTAR_SEAWIFS_GAC.20101103.L3m.DAY.CHL.chlor_a.9km.nc\\n', 'SEASTAR_SEAWIFS_GAC.20101104.L3m.DAY.CHL.chlor_a.9km.nc\\n', 'SEASTAR_SEAWIFS_GAC.20101105.L3m.DAY.CHL.chlor_a.9km.nc\\n', 'SEASTAR_SEAWIFS_GAC.20101106.L3m.DAY.CHL.chlor_a.9km.nc\\n', 'SEASTAR_SEAWIFS_GAC.20101107.L3m.DAY.CHL.chlor_a.9km.nc\\n', 'SEASTAR_SEAWIFS_GAC.20101108.L3m.DAY.CHL.chlor_a.9km.nc\\n', 'SEASTAR_SEAWIFS_GAC.20101109.L3m.DAY.CHL.chlor_a.9km.nc\\n', 'SEASTAR_SEAWIFS_GAC.20101110.L3m.DAY.CHL.chlor_a.9km.nc\\n', 'SEASTAR_SEAWIFS_GAC.20101111.L3m.DAY.CHL.chlor_a.9km.nc\\n', 'SEASTAR_SEAWIFS_GAC.20101112.L3m.DAY.CHL.chlor_a.9km.nc\\n', 'SEASTAR_SEAWIFS_GAC.20101113.L3m.DAY.CHL.chlor_a.9km.nc\\n', 'SEASTAR_SEAWIFS_GAC.20101114.L3m.DAY.CHL.chlor_a.9km.nc\\n', 'SEASTAR_SEAWIFS_GAC.20101115.L3m.DAY.CHL.chlor_a.9km.nc\\n', 'SEASTAR_SEAWIFS_GAC.20101116.L3m.DAY.CHL.chlor_a.9km.nc\\n', 'SEASTAR_SEAWIFS_GAC.20101117.L3m.DAY.CHL.chlor_a.9km.nc\\n', 'SEASTAR_SEAWIFS_GAC.20101118.L3m.DAY.CHL.chlor_a.9km.nc\\n', 'SEASTAR_SEAWIFS_GAC.20101119.L3m.DAY.CHL.chlor_a.9km.nc\\n', 'SEASTAR_SEAWIFS_GAC.20101120.L3m.DAY.CHL.chlor_a.9km.nc\\n', 'SEASTAR_SEAWIFS_GAC.20101121.L3m.DAY.CHL.chlor_a.9km.nc\\n', 'SEASTAR_SEAWIFS_GAC.20101122.L3m.DAY.CHL.chlor_a.9km.nc\\n', 'SEASTAR_SEAWIFS_GAC.20101123.L3m.DAY.CHL.chlor_a.9km.nc\\n', 'SEASTAR_SEAWIFS_GAC.20101124.L3m.DAY.CHL.chlor_a.9km.nc\\n', 'SEASTAR_SEAWIFS_GAC.20101125.L3m.DAY.CHL.chlor_a.9km.nc\\n', 'SEASTAR_SEAWIFS_GAC.20101126.L3m.DAY.CHL.chlor_a.9km.nc\\n', 'SEASTAR_SEAWIFS_GAC.20101127.L3m.DAY.CHL.chlor_a.9km.nc\\n', 'SEASTAR_SEAWIFS_GAC.20101128.L3m.DAY.CHL.chlor_a.9km.nc\\n', 'SEASTAR_SEAWIFS_GAC.20101129.L3m.DAY.CHL.chlor_a.9km.nc\\n', 'SEASTAR_SEAWIFS_GAC.20101130.L3m.DAY.CHL.chlor_a.9km.nc\\n', 'SEASTAR_SEAWIFS_GAC.20101201.L3m.DAY.CHL.chlor_a.9km.nc\\n', 'SEASTAR_SEAWIFS_GAC.20101202.L3m.DAY.CHL.chlor_a.9km.nc\\n', 'SEASTAR_SEAWIFS_GAC.20101203.L3m.DAY.CHL.chlor_a.9km.nc\\n', 'SEASTAR_SEAWIFS_GAC.20101204.L3m.DAY.CHL.chlor_a.9km.nc\\n', 'SEASTAR_SEAWIFS_GAC.20101205.L3m.DAY.CHL.chlor_a.9km.nc\\n', 'SEASTAR_SEAWIFS_GAC.20101206.L3m.DAY.CHL.chlor_a.9km.nc\\n', 'SEASTAR_SEAWIFS_GAC.20101207.L3m.DAY.CHL.chlor_a.9km.nc\\n', 'SEASTAR_SEAWIFS_GAC.20101208.L3m.DAY.CHL.chlor_a.9km.nc\\n', 'SEASTAR_SEAWIFS_GAC.20101209.L3m.DAY.CHL.chlor_a.9km.nc\\n', 'SEASTAR_SEAWIFS_GAC.20101210.L3m.DAY.CHL.chlor_a.9km.nc\\n', 'SEASTAR_SEAWIFS_GAC.20101211.L3m.DAY.CHL.chlor_a.9km.nc']\n",
      "Total matching files: 72\n",
      "Total files needed: 0\n"
     ]
    }
   ],
   "source": [
    "# Create a list of availible files for download\n",
    "def get_all_avail_file_list(file):\n",
    "    file_list = []\n",
    "    with open(file) as f:\n",
    "        for line in f:\n",
    "            # TODO: make more generic \n",
    "            # only add daily data to the list (this is a quick fix for now)\n",
    "            #if \".L3m_DAY_BIOS4_chlor_a_4km.nc\" in line:\n",
    "            if \"DAY\" in line and \"chlor\" in line and \".nc\" in line:\n",
    "                file_list.append(line)\n",
    "    return file_list\n",
    "\n",
    "# Get list of files in filepath\n",
    "def get_files(fp):\n",
    "    # Get file list and sort files by date\n",
    "    files = glob.glob(fp + \"*.nc\")\n",
    "    files.sort()\n",
    "    return files\n",
    "\n",
    "# Strip filepath from file names\n",
    "def format_names(current_files, file_list):\n",
    "    # Strip filepath from file names\n",
    "    current_file_names = []\n",
    "    for ea in current_files:\n",
    "        name = ea.split(\"/\")\n",
    "        current_file_names.append(name[-1])\n",
    "\n",
    "    file_list_names = []\n",
    "    for ea in file_list:\n",
    "        name = ea.strip('\\n')\n",
    "        file_list_names.append(name)\n",
    "\n",
    "    return current_file_names, file_list_names\n",
    "\n",
    "\n",
    "### Get list of availible files that are not already in the specified file path\n",
    "def get_list_files_not_in_path(filepath, file_list):\n",
    "    # Get list of files in filepath\n",
    "    current_files = get_files(filepath)\n",
    "\n",
    "    # Strip file path from file names\n",
    "    current_file_names, file_list_names = format_names(current_files, file_list)\n",
    "\n",
    "    # Get list of files that are not in the specified file path\n",
    "    files_needed = list(set(file_list_names) - set(current_file_names))\n",
    "    files_needed.sort()\n",
    "\n",
    "    return files_needed\n",
    "\n",
    "\n",
    "# Create a list of availible files for download\n",
    "file_list = get_all_avail_file_list(file)\n",
    "print(file_list)\n",
    "\n",
    "# Get list of files that are not in the specified file path\n",
    "files_needed = get_list_files_not_in_path(filepath, file_list)\n",
    "\n",
    "print(\"Total matching files: \" + str(len(file_list)))\n",
    "print(\"Total files needed: \" + str(len(files_needed)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods to download files not in folder with AppKey or Login "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Configure a connection to download data from an Earthdata Login enabled server\n",
    "#   https://urs.earthdata.nasa.gov/documentation/for_users/data_access/python\n",
    "def access_data_login(\n",
    "    username,\n",
    "    password,\n",
    "    filepath,\n",
    "    files_needed,\n",
    "):\n",
    "    # overriding requests.Session.rebuild_auth to mantain headers when redirected\n",
    "    class SessionWithHeaderRedirection(requests.Session):\n",
    "        AUTH_HOST = 'urs.earthdata.nasa.gov'\n",
    "\n",
    "        def __init__(self, username, password):\n",
    "            super().__init__()\n",
    "            self.auth = (username, password)\n",
    "\n",
    "    # Overrides from the library to keep headers when redirected to or from\n",
    "    # the NASA auth host.\n",
    "\n",
    "        def rebuild_auth(self, prepared_request, response):\n",
    "            headers = prepared_request.headers\n",
    "            url = prepared_request.url\n",
    "\n",
    "            if 'Authorization' in headers:\n",
    "                original_parsed = requests.utils.urlparse(response.request.url)\n",
    "                redirect_parsed = requests.utils.urlparse(url)\n",
    "\n",
    "                if (\n",
    "                        original_parsed.hostname != redirect_parsed.hostname\n",
    "                ) and redirect_parsed.hostname != self.AUTH_HOST and original_parsed.hostname != self.AUTH_HOST:\n",
    "                    del headers['Authorization']\n",
    "\n",
    "            return\n",
    "\n",
    "    # Progess Reporting\n",
    "    total_files = len(files_needed)\n",
    "    i = 1\n",
    "    prog = \"\"\n",
    "\n",
    "    # create session with the user credentials that will be used to authenticate access to the data\n",
    "    session = SessionWithHeaderRedirection(username, password)\n",
    "\n",
    "    if not os.path.isdir(filepath):\n",
    "        os.makedirs(filepath)\n",
    "\n",
    "    # list to hold the names of the files that could not be downloaded\n",
    "    bad_files = []\n",
    "    # loop over the files and submit a request for each\n",
    "    for file in files_needed:\n",
    "        if not os.path.isfile(filepath + file):\n",
    "\n",
    "            # build the url from the file name of the file we wish to retrieve\n",
    "            url = \"https://oceandata.sci.gsfc.nasa.gov/ob/getfile/\" + file\n",
    "\n",
    "            # extract the filename from the url to be used when saving the file\n",
    "            filename = url[url.rfind('/') + 1:]\n",
    "\n",
    "            try:\n",
    "                # submit the request using the session\n",
    "                response = session.get(url, stream=True)\n",
    "                if (response.status_code == 200):\n",
    "                    # Progress reporting\n",
    "                    prog = \"Progress: \" + str(i) + \" of \" + str(total_files)\n",
    "                    print(prog, end=\"\\r\")\n",
    "\n",
    "                else:\n",
    "                    print(\"\\n Status: \", response.status_code)\n",
    "\n",
    "                # raise an exception in case of http errors\n",
    "                response.raise_for_status()\n",
    "\n",
    "                # save the file\n",
    "                with open(filepath + filename, 'wb') as fd:\n",
    "                    for chunk in response.iter_content(chunk_size=1024 * 1024):\n",
    "                        fd.write(chunk)\n",
    "\n",
    "            except requests.exceptions.HTTPError as e:\n",
    "                bad_files.append(file)\n",
    "                # handle any errors here\n",
    "                print(e)\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    # Print report of files that failed to download\n",
    "    if (len(bad_files) == 0):\n",
    "        print(\"All files downloaded successfully\")\n",
    "    elif (len(bad_files) > 0):\n",
    "        print(\"Failed to download \", str(len(bad_files)), \" files.\")\n",
    "        print(\"A list of failed files are saved in \",\n",
    "              filepath + \"failed_files.txt\")\n",
    "        print(\"Files that failed to download: \", bad_files)\n",
    "\n",
    "        # Store the bad files in a text file with each file on a seperate line, overwriting the file if it already exists\n",
    "        with open(filepath + \"failed_files.txt\", \"w\") as f:\n",
    "            for s in bad_files:\n",
    "                f.write(s + \"\\n\")\n",
    "\n",
    "    return bad_files\n",
    "\n",
    "\n",
    "### Download files not in folder\n",
    "def access_data_appkey(filepath, files_needed, appKey):\n",
    "\n",
    "    # Create file path if it does not exist\n",
    "    if not os.path.isdir(filepath):\n",
    "        os.makedirs(filepath)\n",
    "\n",
    "    # Progess Reporting\n",
    "    total_files = len(files_needed)\n",
    "    i = 1\n",
    "    prog = \"\"\n",
    "\n",
    "    # Download files\n",
    "    bad_files = []\n",
    "    for f in files_needed:\n",
    "        if not os.path.isfile(filepath + f):\n",
    "            try:\n",
    "                # submit the request\n",
    "                url = \"https://oceandata.sci.gsfc.nasa.gov/ob/getfile/\" + f + \"?appkey=\" + appKey\n",
    "                r = requests.get(url, allow_redirects=True)\n",
    "                # print status or progress\n",
    "                if (r.status_code == 200):\n",
    "                    # Progress reporting\n",
    "                    prog = \"Progress: \" + str(i) + \" of \" + str(total_files)\n",
    "                    print(prog, end=\"\\r\")\n",
    "                else:  # Print status code if not 200\n",
    "                    print(\"\\n Status: \", r.status_code)\n",
    "\n",
    "                # raise an exception in case of http errors\n",
    "                r.raise_for_status()\n",
    "\n",
    "                # save the file\n",
    "                with open(filepath + f, 'wb') as fd:\n",
    "                    for chunk in r.iter_content(chunk_size=1024 * 1024):\n",
    "                        fd.write(chunk)\n",
    "\n",
    "            except requests.exceptions.HTTPError as e:\n",
    "                bad_files.append(f)\n",
    "                # handle any errors here\n",
    "                print(e)\n",
    "\n",
    "        # Progress reporting\n",
    "        #prog = str(i) + \" of \" + str(total_files)\n",
    "        #print(prog, end=\"\\r\")\n",
    "        i += 1\n",
    "\n",
    "    # Print report of files that failed to download\n",
    "    if (len(bad_files) == 0):\n",
    "        print(\"All files downloaded successfully\")\n",
    "    elif (len(bad_files) > 0):\n",
    "        print(\"Failed to download \", str(len(bad_files)), \" files.\")\n",
    "        print(\"A list of failed files are saved in \",\n",
    "              filepath + \"failed_files.txt\")\n",
    "        print(\"Files that failed to download: \", bad_files)\n",
    "\n",
    "        # Store the bad files in a text file with each file on a seperate line, overwriting the file if it already exists\n",
    "        with open(filepath + \"failed_files.txt\", \"w\") as fi:\n",
    "            for s in bad_files:\n",
    "                fi.write(s + \"\\n\")\n",
    "\n",
    "    return bad_files"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download files not already in folder using user choice for AppKey or Login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files downloaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Download files not in folder using user choice for appkey or login\n",
    "if choice == \"1\":\n",
    "    # Download files not in folder using app key\n",
    "    bad_files = access_data_appkey(filepath, files_needed, appKey)\n",
    "    if len(bad_files) > 0:\n",
    "        repeat = input(\n",
    "            \"Would you like to try to download these files again? (y/n): \")\n",
    "        if repeat == \"y\":\n",
    "            # Download files not in folder using login\n",
    "            bad_files = access_data_appkey(filepath, bad_files, appKey)\n",
    "\n",
    "elif choice == \"2\":\n",
    "    # Download files not in folder using login\n",
    "    bad_files = access_data_login(username, password, filepath, files_needed)\n",
    "    if len(bad_files) > 0:\n",
    "        repeat = input(\n",
    "            \"Would you like to try to download these files again? (y/n): \")\n",
    "        if repeat == \"y\":\n",
    "            # Download files not in folder using login\n",
    "            bad_files = access_data_login(username, password, filepath,\n",
    "                                            bad_files)\n",
    "\n",
    "# Delete file list\n",
    "#os.remove(file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
